__module__ = config
model = BiLSTM_CRF
net_file = ./save/bilstm_crf.pt
vocab_file = ./save/vocab.pkl
seg = True
word_hidden = 300
layers = 1
dropout = 0.55
word_dim = 100
predictOut = True
optimizer = adam
epoch = 1
gpu = -1
lr = 0.01
batch_size = 64
eval_batch = 100
tread_num = 4
decay = 0.05
patience = 10
shuffle = True
__doc__ = None
setting:
Namespace(gpu=7, pre_emb=False, seed=10, thread=4)

using GPU device : 7
GPU seed = 10
CPU seed = 10
loading three datasets...
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-train.segpos.conll : sentences:16091，words:714878
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-dev.segpos.conll : sentences:803，words:33332
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-test.segpos.conll : sentences:1910，words:81660
Words : 3635, labels : 98
processing datasets...
finish processing datasets...
BiLSTM_CRF(
  (drop1): Dropout(p=0.55)
  (embedding): Embedding(3635, 100, padding_idx=0)
  (lstm_layer): LSTM(100, 150, batch_first=True, bidirectional=True)
  (out): Linear(in_features=300, out_features=98, bias=True)
  (crf): CRFlayer()
)
Using Adam optimizer...
start to train the model 
==============================Epoch<1>==============================
Computing Train Loss.........
