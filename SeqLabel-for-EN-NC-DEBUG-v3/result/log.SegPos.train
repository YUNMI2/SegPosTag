__module__ = config
model = BiLSTM_CRF
net_file = ./save/bilstm_crf.pt
vocab_file = ./save/vocab.pkl
seg = True
word_hidden = 300
layers = 1
dropout = 0.55
word_dim = 100
predictOut = True
optimizer = adam
epoch = 1
gpu = -1
lr = 0.01
batch_size = 64
eval_batch = 100
tread_num = 4
decay = 0.05
patience = 10
shuffle = True
__doc__ = None
setting:
Namespace(gpu=7, pre_emb=False, seed=10, thread=4)

using GPU device : 7
GPU seed = 10
CPU seed = 10
loading three datasets...
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-train.segpos.conll : sentences:16091，words:714878
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-dev.segpos.conll : sentences:803，words:33332
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-test.segpos.conll : sentences:1910，words:81660
Words : 3635, labels : 98
processing datasets...
finish processing datasets...
BiLSTM_CRF(
  (drop1): Dropout(p=0.55)
  (embedding): Embedding(3635, 100, padding_idx=0)
  (lstm_layer): LSTM(100, 150, batch_first=True, bidirectional=True)
  (out): Linear(in_features=300, out_features=98, bias=True)
  (crf): CRFlayer()
)
Using Adam optimizer...
start to train the model 
==============================Epoch<1>==============================
Computing Train Loss.........
train : loss = 12.6925  precision = 0.8436  recall = 0.8434  f1 = 0.8435
Computing Dev Loss.........
dev   : loss = 13.4980  precision = 0.8302  recall = 0.8281  f1 = 0.8292
Computing Test Loss.........
test  : loss = 13.9428  precision = 0.8225  recall = 0.8252  f1 = 0.8238
Ex best epoch is epoch = 1 ,the dev f1 = 0.8292 the test f1 = 0.8238
save the model...

Start Writing file..........
Finish Writing file!


Start Writing file..........
Finish Writing file!


Start Writing file..........
Finish Writing file!

iter executing time is 0:01:56.543115

train finished with epoch: 1 / 1
best epoch is epoch = 1 ,the dev f1 = 0.8292 the test f1 = 0.8238
2019-08-05 16:59:50.129695
