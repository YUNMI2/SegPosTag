__module__ = config
model = BiLSTM_CRF
net_file = ./save/bilstm_crf.pt
vocab_file = ./save/vocab.pkl
seg = True
word_hidden = 300
layers = 1
dropout = 0.55
word_dim = 100
optimizer = adam
epoch = 1
gpu = -1
lr = 0.01
batch_size = 256
eval_batch = 100
tread_num = 4
decay = 0.05
patience = 10
shuffle = True
__doc__ = None
setting:
Namespace(gpu=5, pre_emb=False, seed=10, thread=4)

using GPU device : 5
GPU seed = 10
CPU seed = 10
loading three datasets...
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-train.segpos.conll : sentences:16091，words:714878
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-dev.segpos.conll : sentences:803，words:33332
/search/odin/zhuyun/Data/WordSeg/CTB5/SEGPOS/ctb5-test.segpos.conll : sentences:1910，words:81660
Words : 3635, labels : 98
processing datasets...
finish processing datasets...
BiLSTM_CRF(
  (drop1): Dropout(p=0.55)
  (embedding): Embedding(3635, 100, padding_idx=0)
  (lstm_layer): LSTM(100, 150, batch_first=True, bidirectional=True)
  (out): Linear(in_features=300, out_features=98, bias=True)
  (crf): CRFlayer()
)
Using Adam optimizer...
start to train the model 
==============================Epoch<1>==============================
Computing Train Loss.........
train : loss = 23.2606  precision = 0.7990  recall = 0.7997  f1 = 0.7993
Computing Dev Loss.........
dev   : loss = 23.1293  precision = 0.7932  recall = 0.7914  f1 = 0.7923
Computing Test Loss.........
test  : loss = 23.9296  precision = 0.7874  recall = 0.7890  f1 = 0.7882
Ex best epoch is epoch = 1 ,the dev f1 = 0.7923 the test f1 = 0.7882
save the model...
iter executing time is 0:01:14.134116

train finished with epoch: 1 / 1
best epoch is epoch = 1 ,the dev f1 = 0.7923 the test f1 = 0.7882
2019-08-05 10:44:42.344580
